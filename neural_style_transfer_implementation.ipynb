{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "neural-style-transfer-implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-dang/neural_style_transfer_implementation/blob/main/neural_style_transfer_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xiw2spYy9j46"
      },
      "source": [
        "# Implementing a Neural Style Transfer Paper with Tensorflow and Keras\n",
        "\n",
        "Link to paper:  \n",
        "https://arxiv.org/pdf/1508.06576v2.pdf\n",
        "\n",
        "In this notebook, I translate their equations into code "
      ],
      "id": "Xiw2spYy9j46"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zHyecs2gOZy"
      },
      "source": [
        "# First, download the images from my github repo and unzip them (:\r\n",
        "%%shell\r\n",
        "\r\n",
        "wget https://github.com/kathleenisrad/style-transfer-implementation/archive/main.zip\r\n",
        "unzip main.zip"
      ],
      "id": "0zHyecs2gOZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHkKar8k9j48"
      },
      "source": [
        "### Import things"
      ],
      "id": "FHkKar8k9j48"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ax0E8S9j48"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow import concat, convert_to_tensor, GradientTape, transpose, reshape, shape, matmul, Variable, zeros\n",
        "from tensorflow.math import reduce_mean\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "id": "D8ax0E8S9j48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoqfQMX9j49"
      },
      "source": [
        "model = VGG19(include_top=False, #no fully connected layers\n",
        "              weights=\"imagenet\",\n",
        "              input_shape=(500, 500, 3),\n",
        "              pooling='avg' #replace max-pooling with average pooling\n",
        "             )"
      ],
      "id": "FUoqfQMX9j49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKjFZTYC9j4-"
      },
      "source": [
        "#We don't want to train the layers because all it's doing is extracting features\n",
        "for layer in model.layers:\n",
        "    layer.trainable=False"
      ],
      "id": "qKjFZTYC9j4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgUacbpL9j4-"
      },
      "source": [
        "### Setting up image preprocessing"
      ],
      "id": "VgUacbpL9j4-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzDOmqtR9j4_"
      },
      "source": [
        "# function to turn the input images into the correct size and then into tensors\n",
        "def image_to_tensor(img_path):\n",
        "    # load an image, reshape to 500x500\n",
        "    image = load_img(img_path, target_size=(500, 500))\n",
        "    # convert to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # return a tensor \n",
        "    return convert_to_tensor(image)"
      ],
      "id": "YzDOmqtR9j4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHWXamAE9j4_"
      },
      "source": [
        "### Setting up the feature extractor"
      ],
      "id": "VHWXamAE9j4_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6GWPORk9j4_"
      },
      "source": [
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])  #outputs for every layers\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict) #make a new model that spits out the output "
      ],
      "id": "j6GWPORk9j4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3AqJblx9j5A"
      },
      "source": [
        "### Defining \"style\" and \"content\" layers\n",
        "\n",
        "> \"Higher layers in the network capture the high-level content in terms of objects and their\n",
        "arrangement in the input image but do not constrain the exact pixel values of the reconstruction ... We therefore refer to the **feature responses in higher layers of the network as the content\n",
        "representation.**\"\n",
        "\n",
        "> \"**Style** can also be defined more locally by including only a **smaller number of lower layers**, leading to different visual experiences\"\n",
        "\n",
        "> \"For the images shown in Fig 2 we matched the **content representation on layer ‘conv4 2’** and the\n",
        "**style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’** (wl =\n",
        "1/5 in those layers, wl = 0 in all other layers) .\""
      ],
      "id": "M3AqJblx9j5A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4brJz6K-9j5A"
      },
      "source": [
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "content_layer = layer_names[13]\n",
        "content_layer"
      ],
      "id": "4brJz6K-9j5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2EOvjF9j5A"
      },
      "source": [
        "#In the paper, they used block2_conv1, but I'm using block2_conv2 because it gave me better results\n",
        "style_layers = layer_names[1:6:4] + layer_names[7:8] + layer_names[12:20:5]\n",
        "style_layers"
      ],
      "id": "fZ2EOvjF9j5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqcMxM_9j5C"
      },
      "source": [
        "### Defining Content Loss\n",
        "\n",
        "> \"So let ~p and ~x be the original image and the image that is generated and P and F their\n",
        "respective feature representation in layer l. We then define the squared-error loss between the\n",
        "two feature representations:\"\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1YoJpKgYnYy3W7s7DYE3CaIERt3HcFJUu)"
      ],
      "id": "7OqcMxM_9j5C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AfGaT9T9j5C"
      },
      "source": [
        "def calc_content_loss(F, P):\n",
        "    L = zeros(shape=())\n",
        "    #I used reduce sum at firt, but it gives you a HUGE number\n",
        "    #used reduce mean instead to keep the number a bit smaller\n",
        "    L += 0.5*reduce_mean((F-P)**2).numpy()\n",
        "    return L"
      ],
      "id": "3AfGaT9T9j5C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7318fQp9j5D"
      },
      "source": [
        "### Defining Style Loss\n",
        "> \"These feature correlations are given by the Gram matrix...\"\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1_EU7qvqPtN4JdySQqE324VtDa3bO0NNy)\n",
        "\n",
        "\n",
        "> \"...generate a texture that matches the style of a given image ... by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the\n",
        "image to be generated...\"\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=11pCUnGZmtTbbjZiGUDLEODymJh6jc3jd)\n",
        "\n",
        "> \"... and the total loss is:\"\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1pSYHMNPOVhF21ydfFQtwb7ZyvgrRhyh5)\n",
        "\n",
        "\n",
        "> \"where wl are weighting factors of the contribution of each layer to the total loss ... (wl =\n",
        "1/5 in those layers)\""
      ],
      "id": "N7318fQp9j5D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPCJbQJb9j5D"
      },
      "source": [
        "#first define gram matrix\n",
        "def calc_gram_matrix(x):\n",
        "    x = transpose(x, (2, 0, 1))\n",
        "    features = reshape(x, (shape(x)[0], -1))\n",
        "    gram = matmul(features, transpose(features))\n",
        "    return gram"
      ],
      "id": "gPCJbQJb9j5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM1Wtfs09j5D"
      },
      "source": [
        "#next, define style loss:\n",
        "\n",
        "def calc_style_loss(G, A):\n",
        "  #calculate the mean squared distance between the two matrices:\n",
        "  # I kinda just chose N and M randomly\n",
        "  N = 32\n",
        "  M = 5000\n",
        "\n",
        "  #style loss equation\n",
        "  E = (1/(4*(N**2)*(M**2)))*sum((G-A)**2) \n",
        "  style_loss = reduce_mean((1/5)*E)\n",
        "  return style_loss"
      ],
      "id": "qM1Wtfs09j5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXOhILiQ9j5E"
      },
      "source": [
        "### Define Total Loss\n",
        "\n",
        "> \"The loss function we minimize is:\"\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1p_Dp3rVrorH0CymOIqD7cknBW806K6td)\n",
        "\n",
        "> \"...where α and β are the weighting factors for content and style reconstruction respectively.\""
      ],
      "id": "gXOhILiQ9j5E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueygvI3Q9j5E"
      },
      "source": [
        "#putting it all together:\n",
        "\n",
        "def calc_total_loss(content_image, style_image, generated_image, alpha=0.2, beta=0.8):  \n",
        "    total_loss = zeros(shape=())\n",
        "    style_loss = zeros(shape=())\n",
        "    \n",
        "    input_tensor = concat([content_image, style_image, generated_image], axis=0)\n",
        "    features = feature_extractor(input_tensor)\n",
        "    \n",
        "    #calculate the content loss:\n",
        "    F = features[content_layer][0,:,:,:]\n",
        "    P = features[content_layer][2,:,:,:]\n",
        "    content_loss = calc_content_loss(F, P)\n",
        "\n",
        "    for layer_name in style_layers:\n",
        "        layer_features = features[layer_name]\n",
        "        style_features = layer_features[1, :, :, :]\n",
        "        generated_features = layer_features[2,: ,: ,:]\n",
        "        \n",
        "        #calculate the gram matrix:\n",
        "        G = calc_gram_matrix(style_features)\n",
        "        A = calc_gram_matrix(generated_features)\n",
        "\n",
        "        #calculate style loss:\n",
        "        style_loss += calc_style_loss(G, A)\n",
        "        \n",
        "    #calculate total loss:\n",
        "    total_loss += alpha*content_loss + beta*style_loss\n",
        "    return total_loss"
      ],
      "id": "ueygvI3Q9j5E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6hP2Z5l9j5F"
      },
      "source": [
        "#calculate gradients\n",
        "def compute_grads(content_array, style_array, generated_array):\n",
        "    with GradientTape() as tape:\n",
        "        loss = calc_total_loss(content_array, style_array, generated_array)\n",
        "        grad = tape.gradient(loss, generated_array)\n",
        "    return loss, grad"
      ],
      "id": "c6hP2Z5l9j5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPBFvIwx9j5F"
      },
      "source": [
        "### Putting it all together into a function!"
      ],
      "id": "RPBFvIwx9j5F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vYuosP3M9j5G"
      },
      "source": [
        "def style_transfer(content_path, style_path, iterations=1000, learning_rate=10, beta_1=0.9, epsilon=0.01):\n",
        "    try: \n",
        "        os.mkdir('./images/')\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "      os.mkdir(f'./images/{style_path[57:-4]}_{content_path[59:-4]}')\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "    #turn images into tensors\n",
        "    style = image_to_tensor(style_path)\n",
        "    content = image_to_tensor(content_path)\n",
        "    generated = Variable(content)\n",
        "\n",
        "    #create an optimizer\n",
        "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, epsilon=epsilon, amsgrad=True, name='Adam')\n",
        "    \n",
        "    #transfer the style over 1001 iterations\n",
        "    for i in range(iterations+1):\n",
        "        loss, grads = compute_grads(content, style, generated)\n",
        "        optimizer.apply_gradients([(grads, generated)])\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(f'-------------------------\\nEpoch: {i} \\nTotal Loss: {loss.numpy()}')\n",
        "            img = generated.numpy().squeeze()\n",
        "            img = np.clip(img, 0, 255)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            cv2.imwrite(f'./images/{style_path[57:-4]}_{content_path[59:-4]}/{style_path[57:-4]}_{content_path[59:-4]}_{i}.jpg', img)\n",
        "            Image.open(f'./images/{style_path[57:-4]}_{content_path[59:-4]}/{style_path[57:-4]}_{content_path[59:-4]}_{i}.jpg')"
      ],
      "id": "vYuosP3M9j5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6VQcqQf9j5G"
      },
      "source": [
        "style_images = os.listdir('./style-transfer-implementation-main/images/style_images')\r\n",
        "style_images"
      ],
      "id": "B6VQcqQf9j5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsybfBlP9j5G"
      },
      "source": [
        "#transfer all styles to cat image\n",
        "# feel free to play around with the number of iterations\n",
        "# to view the image, click on the little folder icon on the very lefthand side \n",
        "# the images are saved under ./images/{style_name}_{content_name}\n",
        "\n",
        "iterations = 500\n",
        "learning_rate = 10\n",
        "\n",
        "for style in style_images:\n",
        "    content_path = './style-transfer-implementation-main/images/content_images/cat.jpg'\n",
        "    style_path = os.path.join('./style-transfer-implementation-main/images/style_images', style)\n",
        "    style_transfer(content_path, style_path, iterations = iterations, learning_rate = learning_rate)"
      ],
      "id": "lsybfBlP9j5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVcv4Pb89j5G"
      },
      "source": [
        "#transfer all styles to zion image\n",
        "\n",
        "iterations = 500\n",
        "learning_rate = 10\n",
        "\n",
        "for style in style_images:\n",
        "    content_path = './style-transfer-implementation-main/images/content_images/zion.jpg'\n",
        "    style_path = os.path.join('./style-transfer-implementation-main/images/style_images', style)\n",
        "    style_transfer(content_path, style_path, iterations = iterations, learning_rate = learning_rate)"
      ],
      "id": "FVcv4Pb89j5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDTw4osg9j5H"
      },
      "source": [
        ""
      ],
      "id": "FDTw4osg9j5H",
      "execution_count": null,
      "outputs": []
    }
  ]
}